import os
import datetime
import asyncio
import uuid
import json
import re
from typing import Any
from dotenv import load_dotenv

from langchain_core.messages import HumanMessage
from langchain_community.callbacks import get_openai_callback

# --- Agent Infrastructure Import ---
from src.agent_infrastructure import create_agent_graph
from src.message_utils import message_to_text
from src.utils import call_llm, run_agent_with_streaming

# Load environment variables
load_dotenv()

# Configuration - Final Forecast Model from .env
FINAL_FORECAST_MODEL = os.getenv("FINAL_FORECAST_MODEL", "gpt-5-mini")
FINAL_FORECAST_REASONING_EFFORT = "high"


def _env_flag(name: str, default: bool) -> bool:
    value = os.getenv(name)
    if value is None:
        return default
    return value.strip().lower() not in {"0", "false", "no", "off"}


FINAL_FORECAST_USE_AGENT = _env_flag("FINAL_FORECAST_USE_AGENT", True)

def _strip_markdown_fences(text: str) -> str:
    clean_text = text.strip()
    if clean_text.startswith("```"):
        clean_text = re.sub(r"^```[a-zA-Z]*\s*\n", "", clean_text)
        clean_text = re.sub(r"\n```(?:\s*)$", "", clean_text)
    return clean_text.strip()


def _extract_json_substring(text: str) -> str:
    match = re.search(r"\{[\s\S]*\}", text)
    return match.group(0).strip() if match else text.strip()


def _escape_newlines_inside_json_strings(text: str) -> str:
    # Some models emit literal newlines inside quoted JSON strings; JSON requires \n escapes.
    out: list[str] = []
    in_string = False
    escaped = False

    for ch in text:
        if in_string:
            if escaped:
                out.append(ch)
                escaped = False
                continue
            if ch == "\\":
                out.append(ch)
                escaped = True
                continue
            if ch == '"':
                out.append(ch)
                in_string = False
                continue
            if ch == "\n":
                out.append("\\n")
                continue
            if ch == "\r":
                # Drop CR and rely on following LF normalization.
                continue
            out.append(ch)
            continue

        out.append(ch)
        if ch == '"':
            in_string = True

    return "".join(out)


def _try_parse_json_dict(content: str) -> dict[str, Any] | None:
    candidates = [
        _strip_markdown_fences(content),
        _extract_json_substring(_strip_markdown_fences(content)),
    ]

    for candidate in candidates:
        if not candidate:
            continue

        # Attempt 1: strict JSON parse.
        try:
            parsed = json.loads(candidate)
            if isinstance(parsed, dict):
                return parsed
        except Exception:
            pass

        # Attempt 2: repair common invalid JSON shape from model output.
        repaired = _escape_newlines_inside_json_strings(candidate)
        repaired = re.sub(r",\s*([}\]])", r"\1", repaired)
        try:
            parsed = json.loads(repaired)
            if isinstance(parsed, dict):
                return parsed
        except Exception:
            pass

    return None


def _coerce_forecast_dict(content: str) -> dict[str, Any] | None:
    parsed = _try_parse_json_dict(content)
    if not parsed:
        return None

    # Occasionally the model returns a JSON object encoded as a string in "rationale".
    if (
        parsed.get("probability") is None
        and parsed.get("percentiles") is None
        and parsed.get("probabilities") is None
    ):
        for key in ("rationale", "raw_output"):
            nested = parsed.get(key)
            if isinstance(nested, str):
                nested_parsed = _try_parse_json_dict(nested)
                if nested_parsed:
                    return nested_parsed

    return parsed


async def generate_final_forecast(
    question_details: dict,
    outside_view_text: str,
    inside_view_text: str,
    prediction_market_data: str = "",
    use_agent: bool | None = None,
) -> dict[str, Any]:
    """Generate a final forecast synthesis with optional agentic execution.

    Args:
        question_details: Dict with title, resolution criteria, etc.
        outside_view_text: The text generated by the Outside View agent.
        inside_view_text: The text generated by the Inside View agent.
        prediction_market_data: Optional text summary of relevant prediction markets.
        use_agent: Toggle for agentic execution. If None, uses FINAL_FORECAST_USE_AGENT env flag.

    Returns:
        The final synthesized forecast commentary and probability/CDF.
    """
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    question_type = question_details.get("type", "binary")
    
    # Select prompt based on type
    if question_type == "binary":
        prompt_file = "final_forecast_binary.txt"
    elif question_type == "multiple_choice":
        prompt_file = "final_forecast_mc.txt"
    elif question_type in ["numeric", "discrete"]:
        prompt_file = "final_forecast_numeric.txt"
    else:
        # Fallback to binary or generic if unknown
        prompt_file = "final_forecast_binary.txt"

    from src.utils import read_prompt
    prompt_template = read_prompt(prompt_file)
    
    # Extract details
    title = question_details.get("title", "")
    resolution_criteria = question_details.get("resolution_criteria", "")
    fine_print = question_details.get("fine_print", "")

    # Prepare formatting args
    # Inject prediction market data if available
    market_context = ""
    if prediction_market_data and "No relevant" not in prediction_market_data:
        market_context = f"\n\n### Prediction Market Data\n{prediction_market_data}\n"

    # Common args
    fmt_args = {
        "title": title,
        "resolution_criteria": resolution_criteria,
        "fine_print": fine_print,
        "outside_view_text": outside_view_text,
        "inside_view_text": inside_view_text + market_context,
        "today": today,
    }
    
    # Type-specific args
    if question_type == "multiple_choice":
        options = question_details.get("options", [])
        fmt_args["options"] = ", ".join([str(o) for o in options]) if isinstance(options, list) else str(options)
    elif question_type in ["numeric", "discrete"]:
        fmt_args["units"] = question_details.get("unit", "") or "Not stated"
        scaling = question_details.get("scaling", {})
        fmt_args["lower_bound"] = scaling.get("range_min", "")
        fmt_args["upper_bound"] = scaling.get("range_max", "")
        fmt_args["zero_point"] = scaling.get("zero_point", "")

    try:
        prompt = prompt_template.format(**fmt_args)
    except KeyError as e:
        print(f"Warning: Missing key {e} in final forecast prompt. Formatting with partial args.")
        # Fallback: try formatting with what we have, leaving missing keys
        # valid for simple format strings, but risky. 
        # Better: just use the keys we have and ignore errors strictly? 
        # For now, let's assume the template matches usage.
        prompt = prompt_template # Fallback to raw template if formatting fails catastrophically

    resolved_use_agent = FINAL_FORECAST_USE_AGENT if use_agent is None else use_agent
    if resolved_use_agent:
        initial_state = {"messages": [HumanMessage(content=prompt)]}
        app = create_agent_graph(model_name=FINAL_FORECAST_MODEL)
        question_id = question_details.get("id", "unknown")
        run_config = {
            "configurable": {
                "thread_id": f"final-{question_id}-{uuid.uuid4().hex}",
            }
        }

        with get_openai_callback() as cb:
            final_output = await run_agent_with_streaming(
                app,
                initial_state,
                run_config,
                label="final_forecast",
            )
            from src.token_cost import print_token_usage
            print_token_usage(cb, FINAL_FORECAST_MODEL, final_output, component="final_forecast")

        last_message = final_output["messages"][-1]
        content = message_to_text(last_message)
    else:
        print("Final forecast running with agent disabled (analysis-only mode).")
        content = await call_llm(
            prompt=prompt,
            model=FINAL_FORECAST_MODEL,
            temperature=0.2,
            reasoning_effort=FINAL_FORECAST_REASONING_EFFORT,
            component="final_forecast",
        )
    
    parsed = _coerce_forecast_dict(content)
    if parsed is not None:
        return parsed

    print("Error parsing final forecast JSON: unable to coerce model output")
    print(f"Raw content: {content}")
    # Return a robust fallback structure
    return {
        "rationale": content,
        "error": "Failed to parse structured output",
        "raw_output": content,
    }

if __name__ == "__main__":
    # Test block
    from src.metaculus_utils import get_post_details
    post_details = get_post_details(41851) # Example question
    q = post_details["question"]
    
    # Dummy views for testing
    ov = "Historically, similar events happen 5% of the time."
    iv = "Current geopolitical tensions suggest a higher risk, maybe 10%."
    
    async def _test():
        print(f"Generating final forecast for: {q['title']}")
        result = await generate_final_forecast(q, ov, iv)
        print("\n" + "#" * 80)
        print("Final Forecast\n" + "#" * 80)
        print(result)

    asyncio.run(_test())
