import os
import datetime
import asyncio
from dotenv import load_dotenv

from langchain_core.messages import HumanMessage
from langchain_community.callbacks import get_openai_callback

# --- Agent Infrastructure Import ---
from src.agent_infrastructure import create_agent_graph

# Load environment variables
load_dotenv()

# Configuration - Final Forecast Model from .env
FINAL_FORECAST_MODEL = os.getenv("FINAL_FORECAST_MODEL", "gpt-5-mini")

# Initialize Graph with the final forecast model
app = create_agent_graph(model_name=FINAL_FORECAST_MODEL)

async def generate_final_forecast(
    question_details: dict,
    outside_view_text: str,
    inside_view_text: str,
    prediction_market_data: str = "",
) -> str:
    """Generate a final forecast synthesis using a frontier model agent.

    Args:
        question_details: Dict with title, resolution criteria, etc.
        outside_view_text: The text generated by the Outside View agent.
        inside_view_text: The text generated by the Inside View agent.
        prediction_market_data: Optional text summary of relevant prediction markets.

    Returns:
        The final synthesized forecast commentary and probability/CDF.
    """
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    question_type = question_details.get("type", "binary")
    
    # Select prompt based on type
    if question_type == "binary":
        prompt_file = "final_forecast_binary.txt"
    elif question_type == "multiple_choice":
        prompt_file = "final_forecast_mc.txt"
    elif question_type in ["numeric", "discrete"]:
        prompt_file = "final_forecast_numeric.txt"
    else:
        # Fallback to binary or generic if unknown
        prompt_file = "final_forecast_binary.txt"

    from src.utils import read_prompt
    prompt_template = read_prompt(prompt_file)
    
    # Extract details
    title = question_details.get("title", "")
    resolution_criteria = question_details.get("resolution_criteria", "")
    fine_print = question_details.get("fine_print", "")

    # Prepare formatting args
    # Inject prediction market data if available
    market_context = ""
    if prediction_market_data and "No relevant" not in prediction_market_data:
        market_context = f"\n\n### Prediction Market Data\n{prediction_market_data}\n"

    # Common args
    fmt_args = {
        "title": title,
        "resolution_criteria": resolution_criteria,
        "fine_print": fine_print,
        "outside_view_text": outside_view_text,
        "inside_view_text": inside_view_text + market_context,
        "today": today,
    }
    
    # Type-specific args
    if question_type == "multiple_choice":
        options = question_details.get("options", [])
        fmt_args["options"] = ", ".join([str(o) for o in options]) if isinstance(options, list) else str(options)
    elif question_type in ["numeric", "discrete"]:
        fmt_args["units"] = question_details.get("unit", "") or "Not stated"
        scaling = question_details.get("scaling", {})
        fmt_args["lower_bound"] = scaling.get("range_min", "")
        fmt_args["upper_bound"] = scaling.get("range_max", "")
        fmt_args["zero_point"] = scaling.get("zero_point", "")

    try:
        prompt = prompt_template.format(**fmt_args)
    except KeyError as e:
        print(f"Warning: Missing key {e} in final forecast prompt. Formatting with partial args.")
        # Fallback: try formatting with what we have, leaving missing keys
        # valid for simple format strings, but risky. 
        # Better: just use the keys we have and ignore errors strictly? 
        # For now, let's assume the template matches usage.
        prompt = prompt_template # Fallback to raw template if formatting fails catastrophically

    initial_state = {"messages": [HumanMessage(content=prompt)]}
    
    with get_openai_callback() as cb:
        final_output = await app.ainvoke(initial_state)
        from src.token_cost import print_token_usage
        print_token_usage(cb, FINAL_FORECAST_MODEL, final_output, component="final_forecast")
    
    last_message = final_output["messages"][-1]
    content = last_message.content
    
    # Parse JSON
    import json
    import re
    
    try:
        # cleanup markdown code blocks
        clean_content = content.strip()
        if clean_content.startswith("```"):
            clean_content = re.sub(r"^```[a-zA-Z]*\n", "", clean_content)
            clean_content = re.sub(r"\n```$", "", clean_content)
        
        data = json.loads(clean_content)
        return data
    except Exception as e:
        print(f"Error parsing final forecast JSON: {e}")
        print(f"Raw content: {content}")
        # Return a robust fallback structure
        return {
            "rationale": content,
            "error": "Failed to parse structured output",
            "raw_output": content
        }

if __name__ == "__main__":
    # Test block
    from src.metaculus_utils import get_post_details
    post_details = get_post_details(41851) # Example question
    q = post_details["question"]
    
    # Dummy views for testing
    ov = "Historically, similar events happen 5% of the time."
    iv = "Current geopolitical tensions suggest a higher risk, maybe 10%."
    
    async def _test():
        print(f"Generating final forecast for: {q['title']}")
        result = await generate_final_forecast(q, ov, iv)
        print("\n" + "#" * 80)
        print("Final Forecast\n" + "#" * 80)
        print(result)

    asyncio.run(_test())
